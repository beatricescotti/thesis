\chapter{Machine Learning in a nutshell} 

Machine learning is a subfield of artificial intelligence that focuses on the development of algorithms and statistical models that enable computers to perform tasks without explicit instructions. Instead, these systems learn from data, identifying patterns and making decisions based on the information they have been trained on.
Machine learning consists of designing efficient and accurate prediction algorithms. More generally, learning techniques are data-drivem methods combining fundamental concepts in computer science with ideas from statistics, probability and optimization.
\subsubsection{Types of tasks}
The following are some standard types of tasks in machine learning:
\begin{itemize}
    \item \textbf{Classification:} Assigning labels to data points based on learned patterns (e.g., e-mail spam detection).
    \item \textbf{Regression:} Predicting continuous values based on input features (e.g., predicting house prices). In regression, the penalty for an incorrect prediction depends on the magnitude of the difference between the true and predicted values, in contrast with classification problem, where there is typically no notion of closeness between various categories. 
    \item \textbf{Object detection:} Identifying and localizing objects within images or videos (e.g., detecting tumours in lung CT scans).
    \item \textbf{Clustering:} Grouping similar data points together without predefined labels.
    \item \textbf{Anomaly detection:} Identifying unusual patterns that do not conform to expected behavior.
    \item \textbf{Ranking}: Ordering items based on their relevance or importance.
\end{itemize}

Algorithms that solve a learning task based on semantically annotated historical data are said to operate in a \textbf{supervised learning} mode. In contrast, algorithms that use data without any semantic annotation are said to operate in an \textbf{unsupervised learning} mode. In the latter case, the algorithm is expected to discover patterns in the data without any prior knowledge of the labels or categories.
In this thesis I'll mainly focus on supervised learning. 

\textbf{Label set: } We use \( Y \) to denote the set of all possible labels for a data point of a given learning problem. Note that the labels can be of two different types: \text{categorical} labels, which are discrete and finite and define classification problems, and \text{continuous} labels, which can take any value in a continuous range and define regression problems.

\section{Neural networks and deep learning} 
Neural networks are a class of machine learning algorithms inspired by the structure and function of the human brain. They consist of interconnected nodes (neurons) that process information in layers. Deep learning refers to the use of neural networks with many layers (deep neural networks) to model complex patterns in large datasets. This approach has led to significant advancements in areas such as image recognition, natural language processing, and game playing.

\section{YOLO: You Only Look Once}
Object detection is a task that involves identifying and classifying objects present in images or videos. Initially, object detection was approached as a pipeline consisting of three main steps: proposal generation, feature extraction and region classification. However, this approach was computationally expensive and often led to suboptimal results.
The emergence of deep learning brought a significant change in object detection, with deep convolutional neural networks (CNNs) playing a crucial role in this transformation. CNNs are designed to automatically learn hierarchical features from raw pixel data, eliminating the need for manual feature engineering. This shift allowed for more efficient and accurate object detection systems.

Currently, deep learning-based object detection frameworks can be classified in two families: 
\begin{itemize}
    \item \textbf{Two-stage detectors:} These methods first generate region proposals and then classify them. Examples include R-CNNs (Region-based Convolutional Neural Networks), that first generate region proposals using a selective search algorithm and then extracts features from these regions using a CNN; the extracted features are then fed into an SVM for object classification. 
    \item \textbf{One-stage detectors:} These methods perform detection in a single pass, directly predicting bounding boxes and class probabilities. Examples include YOLO (You Only Look Once), which exists in eleven versions.The YOLO models are popular for their accuracy and compact size. It is a state-of-the-art model that could be trained on any hardware. YOLOv8, in particular, was developed by Ultralytics and introduced on January 2023. It is used to detect objects in images, classify images and distinguish onjects from each other. 

\end{itemize}

\subsection{Architecture of YOLOv8}
The YOLOv8 architecture is composed of two major parts, namely the \textbf{backbone} and the \textbf{head}, both of which use a fully convolutional neural network. 
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{immagini/cap2/yolo_architecture.png}
    \caption{Architecture of YOLOv8. The backbone extracts features from the input image, while the head predicts bounding boxes and class probabilities.}
    \label{fig:yolo_architecture}
\end{figure}
\begin{itemize}
    \item The \textbf{backbone} is responsible for extracting features from the input image. It consists of a modified version of the CSPDarknet53 architecture, which has 53 convolutional layers and employs a technique called cross-stage partial connections to enhance the transmission of information across the various levels of the network. The convolutional layers are organized in a sequential manner to extract relevant features from the input image.
    \item The \textbf{head} is responsible for predicting bounding boxes and class probabilities. It consists of a series of convolutional layers that take the features extracted by the backbone and apply additional operations to predict the bounding boxes and class probabilities for each object in the image. The head uses a technique called anchor boxes to handle objects of different sizes and aspect ratios.
\end{itemize}


The YOLOv8 framework can be used to perform computer vision tasks such as detection, segmentation, classification and pose estimation and comes with pre-trained models for each task. For detection, the models are pre trained on the COCO dataset, while for classification on ImageNet dataset. 
There are different versions of YOLOv8, each designed for different tasks and with different architectures. The most common versions are YOLOv8n, YOLOv8s, YOLOv8m, YOLOv8l and YOLOv8x, where the letter indicates the size of the model (n for nano, s for small, m for medium, l for large and x for extra large). The larger the model, the more parameters it has and the more computational resources it requires to train and run.

\section{Object Detection Metrics}
A metric is a function that quantifies and evaluates the performance of a model. In the context of object detection, metrics are used to assess how well a model can detect and classify objects in images. 

\subsection*{Intersection over Union (IoU)}
When we talk about object detection, we often refer to the concept of \textbf{bounding boxes}. A bounding box is a rectangular box that is used to define the location of an object in an image, typically represented by its top-left corner coordinates (x, y), width, and height, or top-left and bottom-right coordinates. The bounding box is used to localize the object within the image and is often used in conjunction with a classification label to identify the object.
Intersection over Union (IoU) is a metric used to evaluate the accuracy of the bounding box detection. It measures the overlap between the predicted and the ground truth box. The IoU is calculated as follows:
\begin{equation}
    IoU = \frac{Area_{Intersection}}{Area_{Union}} = \frac{A_{pred} \cap A_{gt}}{A_{pred} \cup A_{gt}}
\end{equation}
The IoU value ranges from 0 to 1, where 0 indicates no overlap and 1 indicates perfect overlap. A common threshold for considering a detection as a true positive is an IoU of 0.5 or higher.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.45\textwidth]{immagini/cap2/iou.png}
    \caption{}
    \label{fig:iou}
\end{figure}

\begin{algorithm}
    \caption{Intersection over Union (IoU) between two bounding boxes}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Boxes $A = (x_A, y_A, w_A, h_A)$ and $B = (x_B, y_B, w_B, h_B)$
        
        \State \textbf{Compute bottom-right corners:}
        \Statex \hspace{\algorithmicindent} $x2_A = x_A + w_A$, \quad $y2_A = y_A + h_A$
        \Statex \hspace{\algorithmicindent} $x2_B = x_B + w_B$, \quad $y2_B = y_B + h_B$
        
        \State \textbf{Compute intersection rectangle:}
        \Statex \hspace{\algorithmicindent} $x_{\text{int}}^{1} = \max(x_A, x_B)$, \quad $y_{\text{int}}^{1} = \max(y_A, y_B)$
        \Statex \hspace{\algorithmicindent} $x_{\text{int}}^{2} = \min(x2_A, x2_B)$, \quad $y_{\text{int}}^{2} = \min(y2_A, y2_B)$

        \State \textbf{Compute intersection area:}
        \Statex \hspace{\algorithmicindent} $w_{\text{int}} = \max(0, x_{\text{int}}^{2} - x_{\text{int}}^{1})$
        \Statex \hspace{\algorithmicindent} $h_{\text{int}} = \max(0, y_{\text{int}}^{2} - y_{\text{int}}^{1})$
        \Statex \hspace{\algorithmicindent} $A_{\text{int}} = w_{\text{int}} \times h_{\text{int}}$
        
        \State \textbf{Compute areas of each box:}
        \Statex \hspace{\algorithmicindent} $A_A = w_A \times h_A$, \quad $A_B = w_B \times h_B$
        
        \State \textbf{Compute union area:}
        \Statex \hspace{\algorithmicindent} $A_{\text{union}} = A_A + A_B - A_{\text{int}}$
        
        \State \textbf{Return:} $\text{IoU} = \dfrac{A_{\text{int}}}{A_{\text{union}}}$
    \end{algorithmic}
\end{algorithm}

\subsection*{Precision and Recall}
Precision and recall are fundamental metrics used to evaluate the performance of object detection models. These metrics provide valuable insights into the model's ability to identify objects of interest within images. 
Let's define some fundamental concepts before diving into precision and recall:
\begin{itemize}
    \item \textbf{True Positive (TP):} These are istances where the model currently identifies and localize objectys, and the intersection over union (IoU) score between the predicted and the ground truth bounding box is equal or greater then a specified treshold.
    \item \textbf{False Positive (FP):} These are cases where the model incorrectly identifies an object that does not exist in the ground truth or where the predicted box has an IoU score below tha define treshold.
    \item \textbf{False Negative (FN):} These rapresent instances where the model fails to detect an object that is present in the ground truth. This means that the model has not detected an object that is present in the image, resulting in a missed detection.
    \item \textbf{True Negative (TN):} Not applicable in object detection. It represents correcly rejecting the absence of objects, but in object detection, the goal is to detect objects rather than the absence of them.
\end{itemize}

\begin{figure}[!ht]
\centering
\begin{minipage}[t]{0.3\textwidth}
    \centering
    \includegraphics[width=\linewidth]{immagini/cap2/TP.png}
    \caption{The object is there, and the model detects it, with an IoU above treshold.}
    \label{fig:TP}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth}
    \centering
    \includegraphics[width=\linewidth]{immagini/cap2/FP.png}
    \caption{The object is there, but the predicted box has an IoU below the treshold.}
    \label{fig:FP}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth}
    \centering
    \includegraphics[width=\linewidth]{immagini/cap2/FN.png}
    \caption{The object is there, and the model doesn't detect it. The ground truth object has no prediction.}
    \label{fig:FN}
\end{minipage}
\caption{Ground truth box is the green rectangle, while the predicted box is the red rectangle.}
\label{fig:TP_FP_FN}
\end{figure}

Let's not define the core metrics: 
\begin{itemize}
    \item \textbf{Precision}: Is a critical metric in model evaluation as it serves to quantify the accuracy of the positive predictions made by the model. It specifically assesses how well the model distinguishes true objects from false positives. In essence, precision provides insight into the model’s ability to make positive predictions that are indeed accurate. A high precision score indicates that the model is skilled at avoiding false positives and provides reliable positive predictions.
    \begin{equation}
        Precision = \frac{TP}{TP + FP}
    \end{equation}

    \item \textbf{Recall}: also known as sensitivity or true positive rate, is another essential metric used in evaluating model performance, especially in object detection tasks. Recall measures the model’s capability to capture all relevant objects in the image. In essence, recall assesses the model’s completeness in identifying objects of interest. A high recall score indicates that the model effectively identifies most of the relevant objects in the data.
    \begin{equation}
        Recall = \frac{TP}{TP + FN}
    \end{equation}
\end{itemize}
We can define the \textbf{Average Precision (AP)} as the area under the precision-recall curve, which is obtained by plotting precision against recall for different confidence thresholds \footnote{The confidence threshold is a tunable parameter used to filter out low-confidence predictions in object detection tasks. Each predicted bounding box is assigned a confidence score, typically representing the estimated probability that the detection both contains an object and correctly identifies its class. Varying this threshold affects the trade-off between precision and recall: lower thresholds increase recall but may introduce more false positives, whereas higher thresholds improve precision at the cost of reduced recall.}. The AP is a single value that summarizes the model's performance across different levels of precision and recall. It is calculated as follows:
\begin{equation}
    AP = \int_{0}^{1} P_{\alpha}(box_{gt}, box_{pred}) dR_{\alpha}
\end{equation}
where \( P_{\alpha} \) is the precision at a given IoU threshold \( \alpha \) and \( R_{\alpha} \) is the recall at the same threshold.
Precision signifies the accuracy of the model's positive predictions, while recall quantifies the model's ability to succesfully identify all relevant objects. AP achieves a harmonious balance between false positive and false negatives, providing a comprehensive evaluation of the model's performance. 

\subsection*{Mean Average Precision (mAP)}
Mean Average Precision (mAP) extends the concept of Average Precision (AP) by providing a global summary of a model’s performance across multiple object classes and, in some cases, multiple localization thresholds. While AP evaluates the precision-recall trade-off for a single class — by varying the confidence threshold and computing the area under the resulting precision-recall curve — mAP averages these AP values to reflect the model’s overall detection capabilities.
\begin{equation}
    mAP = \frac{1}{N} \sum_{i=1}^{N} AP_i
\end{equation}
where \( N \) is the number of classes and \( AP_i \) is the Average Precision for class \( i \).

In practice, several versions of mAP are commonly reported. mAP@0.5 refers to the mean AP calculated at a fixed Intersection over Union (IoU) threshold of 0.5. mAP@[0.5:0.95], computes the average AP across ten IoU thresholds ranging from 0.5 to 0.95 (in increments of 0.05), providing a more rigorous and comprehensive evaluation that accounts for both classification and localization accuracy.
\begin{equation}
    mAP@50 = \frac{1}{N} \sum_{k=1}^{N} AP_K@50
\end{equation}
\begin{equation}
    mAP@[50:95] = \frac{1}{N(95-50)}\sum_{\alpha = 50}^{\alpha = 95}\sum_{k=1}^{N} AP_K@\alpha
\end{equation}
where \( N \)  is the number of classes and \(\alpha\) is the IoU threshold.
\section{Transfer learning}
Transfer learning is a machine learning technique that leverages knowledge gained from one task to improve performance on a different but related task. It is particularly useful when the target task has limited labeled data, allowing models to benefit from pre-trained representations learned from larger datasets. Transfer learning can be applied in various domains, including computer vision, natural language processing, and speech recognition.
\section{Curriculum learning}